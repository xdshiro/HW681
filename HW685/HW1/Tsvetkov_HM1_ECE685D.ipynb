{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "***ECE685D***\n",
    "\n",
    "**Dima Tsvetkov**\n",
    "\n",
    "**NetID: dt169**\n",
    "\n",
    "Agreement 1) This assignment represents my own work. I did not work on this assignment with\n",
    "others. All coding was done by myself.\n",
    "\n",
    "Agreement 2) I understand that if I struggle with this assignment that I will reevaluate whether\n",
    "this is the correct class for me to take. I understand that the homework only gets harder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Problem 1: Linear regression on a simple dataset(30 pts)***\n",
    "**Part (1)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "      cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n0      540.0    0.0     0.0  162.0               2.5           1040.0   \n1      540.0    0.0     0.0  162.0               2.5           1055.0   \n2      332.5  142.5     0.0  228.0               0.0            932.0   \n3      332.5  142.5     0.0  228.0               0.0            932.0   \n4      198.6  132.4     0.0  192.0               0.0            978.4   \n...      ...    ...     ...    ...               ...              ...   \n1025   276.4  116.0    90.3  179.6               8.9            870.1   \n1026   322.2    0.0   115.6  196.0              10.4            817.9   \n1027   148.5  139.4   108.6  192.7               6.1            892.4   \n1028   159.1  186.7     0.0  175.6              11.3            989.6   \n1029   260.9  100.5    78.3  200.6               8.6            864.5   \n\n      fineaggregate  age  csMPa  \n0             676.0   28  79.99  \n1             676.0   28  61.89  \n2             594.0  270  40.27  \n3             594.0  365  41.05  \n4             825.5  360  44.30  \n...             ...  ...    ...  \n1025          768.3   28  44.28  \n1026          813.4   28  31.18  \n1027          780.0   28  23.70  \n1028          788.9   28  32.77  \n1029          761.5   28  32.40  \n\n[1030 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cement</th>\n      <th>slag</th>\n      <th>flyash</th>\n      <th>water</th>\n      <th>superplasticizer</th>\n      <th>coarseaggregate</th>\n      <th>fineaggregate</th>\n      <th>age</th>\n      <th>csMPa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1025</th>\n      <td>276.4</td>\n      <td>116.0</td>\n      <td>90.3</td>\n      <td>179.6</td>\n      <td>8.9</td>\n      <td>870.1</td>\n      <td>768.3</td>\n      <td>28</td>\n      <td>44.28</td>\n    </tr>\n    <tr>\n      <th>1026</th>\n      <td>322.2</td>\n      <td>0.0</td>\n      <td>115.6</td>\n      <td>196.0</td>\n      <td>10.4</td>\n      <td>817.9</td>\n      <td>813.4</td>\n      <td>28</td>\n      <td>31.18</td>\n    </tr>\n    <tr>\n      <th>1027</th>\n      <td>148.5</td>\n      <td>139.4</td>\n      <td>108.6</td>\n      <td>192.7</td>\n      <td>6.1</td>\n      <td>892.4</td>\n      <td>780.0</td>\n      <td>28</td>\n      <td>23.70</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>159.1</td>\n      <td>186.7</td>\n      <td>0.0</td>\n      <td>175.6</td>\n      <td>11.3</td>\n      <td>989.6</td>\n      <td>788.9</td>\n      <td>28</td>\n      <td>32.77</td>\n    </tr>\n    <tr>\n      <th>1029</th>\n      <td>260.9</td>\n      <td>100.5</td>\n      <td>78.3</td>\n      <td>200.6</td>\n      <td>8.6</td>\n      <td>864.5</td>\n      <td>761.5</td>\n      <td>28</td>\n      <td>32.40</td>\n    </tr>\n  </tbody>\n</table>\n<p>1030 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_raw = pd.read_csv('Concrete_Data_Yeh.csv')\n",
    "XY_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Processing the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X = XY_raw.iloc[:, :-1]\n",
    "y = XY_raw.iloc[:, -1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's find the derivatives of MSE loss function with respect to weights (including the bias $\\beta_0$).\n",
    "$$MSE=\\frac{1}{N}\\sum_{i=1}^{N}(Y_i - X_i\\beta - \\beta_0)^2,$$\n",
    "or\n",
    "$$MSE=\\frac{1}{N}||(Y - X\\beta - \\beta_0)||^2.$$\n",
    "To further simplify the expression let's put $\\beta_0$ into the vector $\\beta$. For that, we can add array of 1's into the X as an extra feature. in that case our loss is:\n",
    "$$MSE=\\frac{1}{N}||(Y - X\\beta)||^2.$$\n",
    "Thus, the derivative with respect to weights $\\beta$:\n",
    "$$\\frac{\\partial MSE}{\\partial \\beta}=\\frac{1}{N}X^T(Y - X\\beta).$$\n",
    "Setting the derivative of MSE to zero (in order to get the minimum) and rearranging it, we can get:\n",
    "$$\\frac{1}{N}X^T(Y - X\\beta)=0,$$\n",
    "$$X^TY=X^TX\\beta,$$\n",
    "and the expression for $\\beta$ is\n",
    "$$\\beta=(X^TX)^{-1}X^TY.$$\n",
    "Let's implement these calculations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# implementing the bios term into the X array\n",
    "X_new = X.copy()\n",
    "X_new['bias'] = np.ones(len(y))\n",
    "\n",
    "def LR_multi_get_coefficients(X, y):\n",
    "    \"\"\"\n",
    "    getting coefficients for LR using the exact solution of the minimization problem:\n",
    "    $$\\beta=(X^TX)^{-1}X^TY.$$\n",
    "    \"\"\"\n",
    "    XT_X = np.dot(X.T, X)\n",
    "    XT_X_inv = np.linalg.inv(XT_X)\n",
    "    XT_X_inv_XT = np.dot(XT_X_inv, X.T)\n",
    "    beta = np.dot(XT_X_inv_XT, y)\n",
    "    return beta\n",
    "\n",
    "def y_predicted(X, beta):\n",
    "    # calculation of the predicted values using LR\n",
    "    return np.dot(X, beta.T)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's use our solution to predict the values $Y$ and calculate MSE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 predicted values vs the real values:\n",
      "[79.99, 61.89, 40.27, 41.05, 44.3]\n",
      "[53.46346329 53.73475651 56.81258504 67.66368153 60.91205585]\n",
      "********************************************************************************\n",
      "Loss function value: 107.19723607486016\n",
      "********************************************************************************\n",
      "Coefficients β:\n",
      "[ 0.11980433  0.10386581  0.08793432 -0.14991842  0.2922246   0.01808621\n",
      "  0.02019035  0.11422207]\n",
      "Bias: -23.33121358503561\n"
     ]
    }
   ],
   "source": [
    "def MSE(y, y_pred):\n",
    "    # MSE Loss function for LR\n",
    "    dif = y - y_pred\n",
    "    loss = np.dot(dif.T, dif) / len(y)\n",
    "    return loss\n",
    "\n",
    "beta = LR_multi_get_coefficients(X_new, y)\n",
    "y_pred = y_predicted(X_new, beta)\n",
    "\n",
    "print('First 5 predicted values vs the real values:')\n",
    "print(f'{list(y[:5])}\\n{y_pred[:5]}')\n",
    "print('*'*80)\n",
    "print(f'Loss function value: {MSE(y, y_pred)}')\n",
    "print('*'*80)\n",
    "print(f'Coefficients \\u03B2:\\n{beta[:8]}\\nBias: {beta[8]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can do the same with calculation the actual derivatives of MSE function:\n",
    "$$\\frac{\\partial MSE}{\\partial \\beta}=-\\frac{2}{N}||Y - X\\beta||^T X$$\n",
    "and using the gradient descent with appropriate learning rates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def gradient_beta(X, y, beta):\n",
    "    # d MSE/d beta\n",
    "    dif = y - y_predicted(X, beta)\n",
    "    grad = -2 * np.dot(dif.T, X) / len(y)\n",
    "    return grad\n",
    "\n",
    "beta2 = np.zeros(np.shape(X_new)[1])\n",
    "# have to set different learning rates for beta and bias terms, otherwise I need too many steps\n",
    "learning_rate = 0.0000001\n",
    "learning_rate_bias = 0.05\n",
    "for i in range(20000):\n",
    "    gradient = gradient_beta(X_new, y, beta2)\n",
    "    beta2[:8] -= learning_rate * gradient[:8]\n",
    "    beta2[8] -= learning_rate_bias * gradient[8]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see, that the second approach is getting close to the exact solution. To get rid of the small difference in the loss function values we, probably, need to preprocess the data (at least normalize all the features to the same diapason) and play with the learning rates.\n",
    "At least I am happy since it also works."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function value (exact solution):\n",
      "107.19723607486016\n",
      "Loss function value (gradients):\n",
      "108.1293920372973\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = y_predicted(X_new, beta2)\n",
    "print(f'Loss function value (exact solution):\\n{MSE(y, y_pred)}')\n",
    "print(f'Loss function value (gradients):\\n{MSE(y, y_pred2)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part (2)**\n",
    "\n",
    "Let's split the data randomly into the training and test parts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "X_new_shuffles = X_new.copy().sample(frac=1, random_state=1)\n",
    "y_shuffles = y.copy().sample(frac=1, random_state=1)\n",
    "index_75 = int(len(y)*0.75)\n",
    "X_train, X_test = X_new_shuffles[:index_75], X_new_shuffles[index_75:]\n",
    "y_train, y_test = y_shuffles[:index_75], y_shuffles[index_75:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's randomly choose 6 and 7 independent variables for i={7,8} (We will keep the bias for all 3 models with i={7,8,9})."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8] [0 1 3 4 5 6 7 8] [0 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "beta_m1 = np.arange(9)\n",
    "rng=np.random.RandomState(4)\n",
    "beta_m2_no_bias = np.sort(rng.choice(8, 7, replace=False))\n",
    "beta_m2 = np.concatenate((beta_m2_no_bias, [8]))\n",
    "beta_m3_no_bias = np.sort(rng.choice(8, 6, replace=False))\n",
    "beta_m3 = np.concatenate((beta_m3_no_bias, [8]))\n",
    "print(beta_m1, beta_m2, beta_m3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's find coefficients for all 3 models and calculate losses on the test data sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 parameters MSE Loss:  111.10161646985645\n",
      "********************************************************************************\n",
      "8 parameters MSE Loss:  114.18549582522323\n",
      "********************************************************************************\n",
      "7 parameters MSE Loss:  123.97931131450196\n"
     ]
    }
   ],
   "source": [
    "beta1 = LR_multi_get_coefficients(X_train.iloc[:, beta_m1], y_train)\n",
    "y_pred1 = y_predicted(X_test.iloc[:, beta_m1], beta1)\n",
    "print('9 parameters MSE Loss: ', MSE(y_test, y_pred1))\n",
    "print('*'*80)\n",
    "beta2 = LR_multi_get_coefficients(X_train.iloc[:, beta_m2], y_train)\n",
    "y_pred2 = y_predicted(X_test.iloc[:, beta_m2], beta2)\n",
    "print('8 parameters MSE Loss: ', MSE(y_test, y_pred2))\n",
    "print('*'*80)\n",
    "beta3 = LR_multi_get_coefficients(X_train.iloc[:, beta_m3], y_train)\n",
    "y_pred3 = y_predicted(X_test.iloc[:, beta_m3], beta3)\n",
    "print('7 parameters MSE Loss: ', MSE(y_test, y_pred3))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, with this data it seems like that the best result can be achieved with the higher number of independent parameters. I've tried it for several random choices of the parameters (was changing rng=np.random.RandomState(4)). This result still stays for that data set. 9 is better than 8, 8 is better than 7.\n",
    "\n",
    "I believe, models with multiple linear regression that include more independent variables do not always perform better because of the overfitting. However, It seems like the MSE R^2 Error always will be smaller with more independent variables (found it in several places on the internet. For example here https://stats.stackexchange.com/questions/306267/is-mse-decreasing-with-increasing-number-of-explanatory-variables and https://statisticsbyjim.com/regression/interpret-adjusted-r-squared-predicted-r-squared-regression/). But it does not mean it's better to have more variables even tho MSE is decreasing. It's necessary to look at other metrics as well."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
