{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Exploratory Analysis**\n",
    "First, I looked at all the features on kaggle.com. It has a very useful and convenient data representation\n",
    "as diagrams and overall statistics on each feature.\n",
    "\n",
    "I found out that there are 3 useless features that have the\n",
    "exact same value for all of the dots: ['Over18', 'EmployeeCount', 'StandardHours']. So I deleted them.\n",
    "\n",
    "Also, some of the features are categorical (not something I can efficiently transform into numbers and order them)\n",
    ": ['BusinessTravel', 'Department', 'EducationField', 'Gender',\n",
    "'JobRole', 'MaritalStatus', 'OverTime']. I relocated them to end of the data array (last columns) in order to easier\n",
    "later analysis.\n",
    "\n",
    "A lot of feature have text values so I changed them all into numbers that make sense. For example:\n",
    "'Non-Travel': 0, 'Travel_Rarely': 0.5, 'Travel_Frequently': 1.\n",
    "\n",
    "After that I rescaled all the features into [0, 1] range (for the validation data I used the coefficients\n",
    "obtained from the training data to make sure I am not \"leaking\" any information to the test array.) I thought\n",
    "it would make the distances on the same scale with each other. I also tried to normalize all the features on each\n",
    "other norms (so their squared sum gives 1) but it ended up working worse.\n",
    "\n",
    "Visually I did not notice anything important from just looking at the data representation on kaggle. So I decided\n",
    "to look at the correlation between all the features left and check if there are more not important ones not carrying\n",
    "any additional information. However, it seemed like all the features have at least some new information (no parameters\n",
    "with very high correlation).\n",
    "\n",
    "One of the most important things I noticed just by looking at the data is that it is very imbalanced. Thus, at some\n",
    "point during my work, I was trying to use oversampling to make the data balance. The initial idea was just to copy\n",
    "some less common examples but then I found some algorithms which can make it more efficient. In the end, I\n",
    "tried the imblearn.over_sampling package to make the data more balance. I used SMOTE and ADASYN algorithms from it.\n",
    "ADASYN worked better for the algorithms I used. I haven't tried the undersampling because I decided that our data\n",
    "is not big enough for that.\n",
    "\n",
    "Moreover, to find which features are 'good' I plotted all the ROC curves for all the individual features\n",
    "using logistic regressions. To achieve this goal, I was also trying to use a new tool for myself -\n",
    "SelectKBest from sklearn.feature_selection with 3 different score functions.\n",
    "Later I was trying to throw away a different number of the least well-performing features based on all of these\n",
    "approaches and their combination.\n",
    "\n",
    "For the same purpose, I also tried SelectFromModel from the same package on some of my best models. This package\n",
    "shows what are the most important features of this particular model. So It helped me to retrain my models throwing\n",
    "away some of the least important features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Models**\n",
    "\n",
    "The first algorithm I chose was the logistic regression from sklearn.linear_model.LogisticRegression. Since this\n",
    "class is my first experience in machine learning usage I decided to select the algorithm I have the most experience\n",
    "with through all our lectures and homework. Moreover, this algorithm is very well-known and has a lot of\n",
    "well-developed instruments in the same library (sklearn). On top of that, it is one of the fastest algorithms\n",
    "and it has a wide variety of parameters I can work with. Logistic regression is much easier to use than a lot of\n",
    "other algorithms. All of that together allowed me not only to study some of the new methods I have never used before\n",
    "(such as SelectFromModel and GridSearchCV) but also I could try a lot of different approaches way easier and faster\n",
    "(for example it was a good way to learn how oversampling works, which parameters should I choose). Moreover,\n",
    "because of how fast it is I practiced and developed some functions and algorithms for the next models I used.\n",
    "Also, since I used logistic regression to study the \"quality\" of the features it was easier to operate with these\n",
    "good features via logistic regression algorithm.\n",
    "\n",
    "The second algorithm I used was a random forest classifier. Besides the similar to the logistic regression benefits, such as\n",
    "well-studied, good implementation in sklearn, ease of use, and simplicity and parametrization, (also, I could\n",
    "a lot of scripts I used for logistic regression) there was one more\n",
    "thing I was guided by. In the logistic regression, I could not but the problem of overfitting that easily and I thought\n",
    "that random forest classifier should be better in terms of overfitting (In the end it did not help that much but that\n",
    "is what I thought because it was easy to understand what some parameters such as tree max depth were actually doing).\n",
    "\n",
    "\n",
    "The last algorithm I used was a Adaboost. Besides all the advantages I mentioned earlier (but both\n",
    "the Adaboost and random forest are not even close to the logistic regression in terms of speed) this method\n",
    "was a good idea to use because I have practiced a lot with decision tree classifiers previously in the\n",
    "random forest algorithm parametrization, so I had some idea of how deep my trees should be,\n",
    "how many nodes and leaves it should have and so on. I decided\n",
    "that I can quickly try this algorithm and see if it works well (it was also 3 times faster\n",
    "than the random forest classifier). In the end, this method was the one\n",
    "that performed the best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Training**\n",
    "\n",
    "For the logistic regression problem I was mainly working with l2 regularization term and using mostly LIBLINEAR solver,\n",
    "and thus, it minimizes for $\\mathbf{\\beta}$ the following cost function:\n",
    "$\\dfrac{1}{n}\\sum_{i=1}^n\\log(1+exp(-y_i\\mathbf{\\beta}^T\\mathbf{x_i}) + \\lambda ||\\mathbf{\\beta}||_2^2$ where\n",
    "$\\lambda$ is the penalty parameter (it's $C$ in sklearn.linear_model.LogisticRegression).\n",
    "In some cases, the discriminant function of the classifier includes a bias term. LIBLINEAR handles this term by\n",
    "augmenting the vector w and each instance xi with an additional dimension:\n",
    "$\\mathbf{\\beta}^T\\leftarrow \\mathbf{\\beta}^T, b, \\mathbf{x}_i^T \\leftarrow \\mathbf{x}_i^T, B$\n",
    "where $B$ is a constant specified by the user. LIBLINEAR uses Automatic parameter selection and it applies\n",
    "the coordinate descent algorithm.\n",
    "For multi-class classification the problem is decomposed in 2 possible way: 1) one-vs-the rest, 2) Crammer & Singer.\n",
    "LIBLINEAR actually can also support the SVM algorithm.\n",
    "(Source (LIBLINEAR site): https://www.csie.ntu.edu.tw/~cjlin/liblinear/)\n",
    "\n",
    "The core principle of adaboost is to fit a group of weak learners (at least slightly above the random guessing) on the\n",
    "data, which being constantly modified with each iteration. After several iterations we combine all the weak learners\n",
    "with obtained weights.\n",
    "Adaboost from sklearn.ensemble implements the algorithm known as AdaBoost-SAMME. This training algorithm can be\n",
    "represented in following steps: 1) initialization of the observation weights $w_i=1/n$; 2) for $m=1$ to $m=M$ do:\n",
    "a) fitting each classifier $T^m$ with the weight $w_m$;\n",
    "b) compute error $\\sum_{i=1}^nw_iI(y_i\\ne T^m(x_i))/\\sum_{i=1}^nw_i$;\n",
    "c) compute coefficients $\\alpha^m=\\log\\dfrac{1-err^m}{err^m}$;\n",
    "d) set $w_i\\leftarrow w_i \\exp(\\alpha^m I(y_i\\ne T^m(x_i)))$;\n",
    "e) Re-normalization of $w_i$.\n",
    "So the final output is the combination of $\\alpha^m T^m$.\n",
    "(Source: Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009.)\n",
    "\n",
    "For the random forest classifier is a black box algorithm which averaging a bunch of decision trees on various\n",
    "sub samples of features. Moreover, the 2nd level of randomness is achieved by training each tree from a sample\n",
    "of the training set selected with replacement. These high level of randomness should help with the decreasing\n",
    "of the overall variance of the final classifier.  The scikit-learn implementation combines classifiers by\n",
    "averaging their probabilistic prediction.\n",
    "(Source: https://scikit-learn.org/ documentation and guides)\n",
    "\n",
    "Logistic regression was by far the fastest method to use. Average time required for 1 training with the\n",
    "whole training data was ~9ms, while for the random forest this time was more than 100 times longer and was ~1s.\n",
    "AdaBoost was slightly faster than random forest - around 0.4s on average. (I haven't used any extra optimization\n",
    "or parallelism features for these measurements)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Hyperparameter Selection**\n",
    "\n",
    "For all the algorithms I was using GridSearchCV and then more precise tuning by hand. The logic\n",
    "behind the idea, I was using to tune the parameters is the following:\n",
    "\n",
    "**Common ideas for all the algorithms:**\n",
    "\n",
    "For the class weights, I tried all the values from 1:1 up to 1:5 and also the \"balanced\" case. In most cases\n",
    "ratio around 1:2.5 - 1:30 worked better than the balanced case. And then I was using\n",
    "oversampling data (with 1:1 ratio between labels).\n",
    "GridSearchCV allows me to see the F1 score for each combination of parameters (by using verbose=3) but for that\n",
    "I had to set n_jobs=1 (otherwise it doesn't work in parallel).\n",
    "\n",
    "First, I set some diapason for all of the possible parameters for each algorithm\n",
    "and looked at the performance for all of them to get (it may take some time but it definitely worth it\n",
    "to get some ideas of how each parameter affects the algorithm). After that, I could cast aside a lot of\n",
    "values for most of the features (if f1 is too low (underfitting) or very close to 1 (overfitting)).\n",
    "\n",
    "Then I was tuning every feature 1 by 1 and looked at the performance on both training and validation data sets\n",
    "trying to make them as high as possible (mostly validation set). As soon as I got some good combinations\n",
    "of parameters I was looking precisely at each parameter with finner steps in a small range around these\n",
    "combinations.\n",
    "\n",
    "Also, for all of the algorithms I tuned the number of folder for the cross-validation and the best value 5-7 folders.\n",
    "\n",
    "**Some specific for each algorithm notes:**\n",
    "\n",
    "Random Forest: For this algorithm I was tuning the following parameters: ['n_estimators', 'criterion',\n",
    "'max_depth', 'min_samples_split', 'min_samples_leaf', 'class_weight', 'max_leaf_nodes', 'max_features'\n",
    "]. The most important parameters were 'max_depth' and 'max_features'.\n",
    "\n",
    "AdaBoost: For the adaboost with the decision tree estimator I already knew where I should look at for\n",
    "'max_depth' and 'max_features' values, since I tuned them in the random forest case. And when I tried some\n",
    "different combinations it turned out that these values should be in the same range. However,\n",
    "tuning adaboost was more complicated because 2 parameters 'learning_rate' and 'n_estimators' affect\n",
    "a lot the accuracy of the algorithm. Even a slight change in any of them may completely change the performances.\n",
    "\n",
    "Logistic regression: Some of the parameters I was tuning were: ['C', 'penalty', 'intercept_scaling', 'solver'].\n",
    "First, I started with solvers and found out that they do not affect f1 score that much so I just stopped on the\n",
    "one which works for both l1 and l2 regularization. Also, l2 shows itself better so I was working mostly with it.\n",
    "\n",
    "Below you can see 2 plots with f1 scores of my final AdaBoost model vs 2 different hyperparameters (learning rate\n",
    "and number of estimators). We can see, that the best values are 0.97 and 104 for the learning rate and\n",
    "the number of estimators, accordingly. Validation data and training data were used.\n",
    "(Black line is for the validation data, red line is for the training data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Data Splits**\n",
    "\n",
    "My first step was to split the training data into 2 parts - the training part and the validation data. Validation data\n",
    "was used to estimate the performance of my final algorithms and it was untouched until the very end. For this\n",
    "splitting, I tried several values: 10%, 15%, 20%, 25%, and 30% for the validation data. In the end, I stopped\n",
    "on the 20% because values lower were not accurate enough to judge my models, and values higher took too much\n",
    "from the training data so it worked worse. For the final model, I tried to use both just training data and\n",
    "full training + validation data.\n",
    "\n",
    "For the cross-validation process, I tried several numbers of folders: from 4 to 10. 10 was definitely too many\n",
    "(probably because the data is not large enough and it is also unbalanced). The best performing value I stopped on\n",
    "was 6-7 folders. It showed the best results in terms of not overfitting the data too much. To get this result\n",
    "I was training my models with different parameters on the training data (using different numbers of folders),\n",
    "checked the average f1 and accuracy values, and then using these models I\n",
    "looked the at f1 and accuracy values for the validation data. If the values for the training data were too high\n",
    "when the same values for the validation data were too low (like 99% vs 50%) I was definitely overfitting the\n",
    "data oo much."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Errors and Mistakes**\n",
    "\n",
    "While working on that project I have several mistakes since I am new to ML:\n",
    "\n",
    "1) I did not preprocess the data the same way for training data and test data. I was not noticing it for quite a\n",
    "while and could not understand why my scores on kaggle were all below 50%.\n",
    "\n",
    "2) For the first couple of days I did not split my training data into the training and validation (test) parts.\n",
    "So I could not check the performance of my models besides looking at the scores on the training data (which was\n",
    "always 1) and checking the results on Kaggle (which was limited to 10 per day). And it slowed me down a lot.\n",
    "\n",
    "3) I did not know that in the GridSearchCV you can increase the number of parallel jobs so all my trainings were\n",
    "very slow (when I set the number of jobs to 16 (I have 16 threads), and the speed increased about 7-8 times).\n",
    "\n",
    "4) I was working with a very low number of features (only used 5-15 of the best features) because I thought it could\n",
    "make my algorithms perform better, but in reality, almost all the best results I got used 31 features (all of the\n",
    "usable ones)\n",
    "\n",
    "5) In the beginning I was not looking at the results of each combination of the parameters and only worked\n",
    "with the best models. Thus, I could not control well what exactly each parameter was doing.\n",
    "\n",
    "6) I was changing too many hyperparameters at the same time trying to catch the best result. In the end, I stopped\n",
    "doing it and started to look at them 1 by 1 and at some small combinations of changes.\n",
    "\n",
    "7) I did not know that Kaggle has an amazing data representation by default so I was trying to plot all the features\n",
    "to see what they are.\n",
    "\n",
    "8) Also, one huge mistake I did - I forgot to add random_state into the oversampling and I was always getting\n",
    "different values when I was learning how to work with imblearn.over_sampling."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Predictive Accuracy**\n",
    "\n",
    "Kaggle nickname: Dakine100500 (Dima Tsvetkov)\n",
    "\n",
    "If we compare results using f1 score on kaggle.com, my best result was achieved with the Adaboost algorithm $F_1=0.7$,\n",
    "my second best was achieved with the logistic regression $F_1=0.66$, and the least success I had with the random forest\n",
    "$F_1=0.59$. However, these numbers correlate a lot with the time I have spent on each algorithm and the order I was\n",
    "working on it. The last algorithm I was trying to optimize was Adaboost and I also spent the most pure time on it\n",
    "compared to 2 others. So I believe, it's definitely possible to achieve better results with other 2. Especially\n",
    "with the random forest algorithm. I was struggling a lot with parameter optimization.\n",
    "\n",
    "For my final model we can look at ROC curve for it (with AUC=)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Code**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}