{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Dima Tsvetkov**\n",
    "\n",
    "**NetID: dt169**\n",
    "\n",
    "**Homework #3**\n",
    "\n",
    "Agreement 1) This assignment represents my own work. I did not work on this assignment with\n",
    "others. All coding was done by myself.\n",
    "\n",
    "Agreement 2) I understand that if I struggle with this assignment that I will reevaluate whether\n",
    "this is the correct class for me to take. I understand that the homework only gets harder."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **1 Convexity (Chudi)**\n",
    "**1.1 Markov’s Inequality**\n",
    "\n",
    "Markov's Inequality:\n",
    "$P(X\\ge \\epsilon)\\le \\dfrac{E(X)}{\\epsilon}$, for any $\\epsilon >0$.\n",
    "\n",
    "By definition, the expectation $E(X)$ for a non-negative $X$:\n",
    "\n",
    "$$E(X)=\\int_0^{\\infty}xf(x)dx$$,\n",
    "\n",
    "splitting the integral into to intervals $[0,\\epsilon]$ and $[\\epsilon, \\infty)$ we can write the following:\n",
    "\n",
    "$$E(X)=\\int_0^{\\epsilon}xf(x)dx + \\int_{\\epsilon}^{\\infty}xf(x)dx\\ge \\int_{\\epsilon}^{\\infty}xf(x)dx$$.\n",
    "\n",
    "Because for $x\\in[\\epsilon, \\infty)$ we can write $x\\ge\\epsilon$,\n",
    "\n",
    "$$E(X)\\ge\\int_{\\epsilon}^{\\infty}xf(x)dx\\ge\\int_{\\epsilon}^{\\infty}\\epsilon f(x)dx\\ge\\epsilon P(X\\ge\\epsilon)$$,\n",
    "\n",
    "or\n",
    "\n",
    "$$P(X\\ge \\epsilon)\\le \\dfrac{E(X)}{\\epsilon}.$$\n",
    "\n",
    "For a discrete $X$ we can simply replace integration $\\int_{x1}^{x2}$ with the\n",
    " summation $\\sum_{n=n_1}^{n=n_2}$ over $x_n$ and the proof above stays the same.\n",
    "\n",
    " We can also use a more simple logic to prove both cases:\n",
    "\n",
    " $$E(X)=P(X<\\epsilon)E(X:X<\\epsilon)+P(X\\ge\\epsilon)E(X:X\\ge\\epsilon),$$\n",
    "\n",
    " since $X\\ge0$: $E(X:X<\\epsilon)\\ge0$, and $P(X<\\epsilon)\\ge0$ (it's a probability). Thus,\n",
    "\n",
    " $$E(X)\\ge P(X\\ge\\epsilon)E(X:X\\ge\\epsilon),$$\n",
    "\n",
    " and because $E(X:X\\ge\\epsilon)\\ge\\epsilon$ (expectation of values larger than $\\epsilon$ is larger than $\\epsilon$)\n",
    "\n",
    "\n",
    "  $$E(X)\\ge P(X\\ge\\epsilon)E(X:X\\ge\\epsilon)\\ge P(X\\ge\\epsilon)\\epsilon,$$\n",
    "\n",
    "  or\n",
    "\n",
    "$P(X\\ge \\epsilon)\\le \\dfrac{E(X)}{\\epsilon}.$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.2  Chebyschev’s Inequality**\n",
    "\n",
    "Chebyschev’s Inequality: $P(|X-\\mu|\\ge\\epsilon)\\le\\dfrac{\\sigma^2}{\\epsilon^2}$, for any $\\epsilon >0$.\n",
    "\n",
    "Let's prove it by using Markov's Inequality for the variable $X_{new}=(X-\\mu)^2$ and $\\epsilon_{new}=\\epsilon^2$:\n",
    "\n",
    "$$P(X_{new}\\ge \\epsilon_{new})\\le \\dfrac{E(X_{new})}{\\epsilon_{new}},$$\n",
    "\n",
    "$$P((X-\\mu)^2\\ge \\epsilon^2)\\le \\dfrac{E((X-\\mu)^2)}{\\epsilon^2}.$$\n",
    "\n",
    "Since $(X-\\mu)^2\\ge \\epsilon^2$ is equivalent to $|X-\\mu|\\ge \\epsilon$ for $\\epsilon\\ge 0$, and by\n",
    "using the definition of the variance $E((X-\\mu)^2)=\\sigma^2$, we can write\n",
    "\n",
    "$$P(|X-\\mu|\\ge\\epsilon)\\le \\dfrac{E((X-\\mu)^2)}{\\epsilon^2}=\\dfrac{\\sigma^2}{\\epsilon^2},$$\n",
    "\n",
    "or\n",
    "\n",
    "$$P(|X-\\mu|\\ge\\epsilon)\\le\\dfrac{\\sigma^2}{\\epsilon^2}.$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.3 Polynomial Markov’s Inequality**\n",
    "\n",
    "Using Markov's Inequality the same way we used in p1.2, we can write the following:\n",
    "\n",
    "$$P(|X-\\mu|\\ge\\epsilon)=P(|X-\\mu|^k\\ge\\epsilon^k)\\le\\dfrac{E(|X-\\mu|^k)}{\\epsilon^k},$$\n",
    "\n",
    "or\n",
    "\n",
    "$$P(|X-\\mu|\\ge\\epsilon)\\le\\dfrac{E(|X-\\mu|^k)}{\\epsilon^k}.$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.4 Chernoff Bound**\n",
    "\n",
    "Using inequality $(X-\\mu)\\ge\\epsilon$ we can write: $\\exp(\\lambda (X-\\mu))\\ge\\exp(\\lambda\\epsilon)$ for any\n",
    "$\\lambda>0$.\n",
    "\n",
    "Now, let's use Markov's Inequality to the random variable $\\exp(\\lambda (X-\\mu))$:\n",
    "\n",
    "$$P(X-\\mu\\ge\\epsilon)=P(\\exp(\\lambda (X-\\mu))\\ge\\exp(\\lambda\\epsilon))\\le\\dfrac{E(\\exp(\\lambda (X-\\mu)))}{\\exp(\\lambda\\epsilon)},$$\n",
    "\n",
    "or\n",
    "\n",
    "$$P(X-\\mu\\ge\\epsilon)\\le\\dfrac{M_{X-\\mu}(\\lambda)}{\\exp(\\lambda\\epsilon)},$$\n",
    "\n",
    "Because it works for any $\\lambda\\ge0$, it also correct for the value $\\lambda$ which gives us\n",
    "the infimum of $\\dfrac{M_{X-\\mu}(\\lambda)}{\\exp(\\lambda\\epsilon)}$. Thus, we can write\n",
    "\n",
    "$$P(X-\\mu\\ge\\epsilon)\\le\\inf_{\\lambda\\ge0}\\dfrac{M_{X-\\mu}(\\lambda)}{\\exp(\\lambda\\epsilon)}.$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.5 Hoeffding’s inequality**\n",
    "\n",
    "The same way we did it in 1.4, we can write following:\n",
    "\n",
    "$$\\dfrac{1}{n}\\sum_{i=1}^nX_i-\\mu\\ge\\epsilon\\Rightarrow\\exp\\left(\\lambda\\left(\\dfrac{1}{n}\\sum_{i=1}^nX_i-\\mu\\right)\\right)\\ge\\exp(\\lambda\\epsilon),$$\n",
    "\n",
    "Now, using Markov's Inequality:\n",
    "\n",
    "$$P\\left(\\dfrac{1}{n}\\sum_{i=1}^nX_i-\\mu\\ge\\epsilon\\right)=P\\left(\\exp\\left(\\dfrac{\\lambda}{n}\\sum_{i=1}^nX_i-\\lambda\\mu\\right)\\ge\\exp(\\lambda\\epsilon)\\right)\\le$$\n",
    "\n",
    "$$\\le\\dfrac{E\\left[\\exp\\left(\\dfrac{\\lambda}{n}\\sum_{i=1}^nX_i-\\lambda\\mu\\right) \\right]}{\\exp(\\lambda\\epsilon)}=\\dfrac{E\\left[\\prod_{i=1}^n\\exp\\left(\\dfrac{\\lambda}{n}\\left(X_i - \\mu \\right)\\right) \\right]}{\\exp(\\lambda\\epsilon)}.$$\n",
    "\n",
    "Because $X_i$ are I.I.D., the expectation value of their product is equal to the product of their expectation:\n",
    "\n",
    "$$\\dfrac{E\\left[\\prod_{i=1}^n\\exp\\left(\\dfrac{\\lambda}{n}\\left(X_i - \\mu \\right)\\right) \\right]}{\\exp(\\lambda\\epsilon)}=\\dfrac{\\prod_{i=1}^nE\\left[\\exp\\left(\\dfrac{\\lambda}{n}\\left(X_i - \\mu \\right)\\right) \\right]}{\\exp(\\lambda\\epsilon)}.$$\n",
    "\n",
    "Now, by applying Hoeffding's Lemma, we ca write\n",
    "\n",
    "$$\\dfrac{\\prod_{i=1}^nE\\left[\\exp\\left(\\dfrac{\\lambda}{n}\\left(X_i - \\mu \\right)\\right) \\right]}{\\exp(\\lambda\\epsilon)}\\le\\dfrac{\\prod_{i=1}^n\\exp\\left( \\dfrac{\\lambda^2(b-a)^2}{8n^2} \\right)}{\\exp(\\lambda\\epsilon)}=$$\n",
    "\n",
    "$$=\\exp\\left( \\dfrac{\\lambda^2(b-a)^2}{8n} -\\lambda\\epsilon \\right).$$\n",
    "\n",
    "To find the minimum value of $\\exp\\left( \\dfrac{\\lambda^2(b-a)^2}{8n} -\\lambda\\epsilon \\right)$ let's calculate\n",
    "the derivative of $\\dfrac{\\lambda^2(b-a)^2}{8n} -\\lambda\\epsilon$ with respect to $\\lambda$:\n",
    "\n",
    "$$\\left(\\dfrac{\\lambda^2(b-a)^2}{8n} -\\lambda\\epsilon \\right)'=\\dfrac{\\lambda(b-a)^2}{4n} -\\epsilon=0,$$\n",
    "\n",
    "so $\\lambda=\\dfrac{4n\\epsilon}{(b-a)^2}$. By substituting the value into the initial expression\n",
    "\n",
    "$$\\exp\\left( \\dfrac{\\lambda^2(b-a)^2}{8n} -\\lambda\\epsilon \\right)|_{\\lambda=\\dfrac{4n\\epsilon}{(b-a)^2}}=\\exp\\left(-\\dfrac{2n\\epsilon^2}{(b-a)^2}\\right),$$\n",
    "\n",
    "and thus, we got one-sided Hoeffding's Inequality:\n",
    "\n",
    "$$P\\left(\\dfrac{1}{n}\\sum_{i=1}^nX_i-\\mu\\ge\\epsilon\\right)\\le\\exp\\left(-\\dfrac{2n\\epsilon^2}{(b-a)^2}\\right).$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.6 Applying concentration inequalities to classifier evaluation**\n",
    "\n",
    "Part 1.\n",
    "\n",
    "Using two-sided Hoeffding's Inequality\n",
    "\n",
    "$$P\\left(\\left|R_S(g)-R(g)\\right| \\leq \\epsilon \\right) \\geq 1- 2\\exp\\left(\\frac{-2n\\epsilon^2}{(b-a)^2}\\right)=1-\\delta,$$\n",
    "\n",
    "taking into account $b-a=1$, we cat write\n",
    "\n",
    "$$\\delta = 2\\exp\\left(\\dfrac{-2n\\epsilon^2}{(b-a)^2}\\right)=2\\exp\\left(-2n\\epsilon^2\\right),$$\n",
    "\n",
    "$$\\ln\\dfrac{\\delta}{2}=-2n\\epsilon^2,$$\n",
    "\n",
    "and thus, the lower boundary for value n is\n",
    "\n",
    "$$n=\\dfrac{\\ln\\dfrac{2}{\\delta}}{2\\epsilon^2}.$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part 2.\n",
    "\n",
    "Chebyshev's Inequaity:\n",
    "\n",
    "$$P(|R_S(g)-R(g)|\\ge\\epsilon)\\le\\dfrac{E(|R_S(g)-R(g)|^2)}{\\epsilon^2},$$\n",
    "\n",
    "by term we have\n",
    "\n",
    "$$R_S=E_D[l_{01}(y,g(x))]=\\mu,\\;\\;\\;R_{S}(g)=\\frac{1}{n}\\sum_{i = 1}^n l_{01}(y_i, g(x_i)),$$\n",
    "\n",
    "and because these are i.i.d. events, which implies that the variance of sum is equal to the sum of variances,\n",
    "we can write\n",
    "\n",
    "$$P(|R_S(g)-R(g)|\\ge\\epsilon)\\le\\dfrac{E(|\\frac{1}{n}\\sum_{i = 1}^n l_{01}(y_i, g(x_i))-\\mu|^2)}{\\epsilon^2}=$$\n",
    "\n",
    "$$=\\dfrac{E(|\\frac{1}{n}\\sum_{i = 1}^n( l_{01}(y_i, g(x_i))-\\mu)|^2)}{\\epsilon^2}=\\dfrac{\\dfrac{1}{n^2}\\sum_{i = 1}^nVar(l_{01}(y_i,g(x_i))-\\mu)}{\\epsilon^2},$$\n",
    "\n",
    "we know, that $Var(l_{01}(y,g(x))-\\mu)=1$, and thus\n",
    "\n",
    "$$P(|R_S(g)-R(g)|\\ge\\epsilon)\\le\\dfrac{\\dfrac{1}{n^2}n}{\\epsilon^2}=\\dfrac{1}{n\\epsilon^2}.$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\\delta=\\dfrac{1}{n\\epsilon^2}\\;\\;\\;\\Rightarrow\\;\\;\\;n=\\dfrac{1}{\\delta\\epsilon^2}.$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part 3.\n",
    "\n",
    "Comparing the lower bound for Hoeffding’s inequality $n_H=\\dfrac{\\ln\\dfrac{2}{\\delta}}{2\\epsilon^2}$ with the\n",
    "lower bound for Chebyschev’s inequality $n_C=\\dfrac{\\dfrac{1}{\\delta}}{\\epsilon^2}$, we can see that with\n",
    "increasing of $\\dfrac{1}{\\delta}$ value $n_H$ growth slower ($\\ln$ compared to linear). So if we want higher\n",
    "probability $1-\\delta$ (lower $\\delta$), we need much more samples in the case of  Chebyschev’s inequality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.7 Application to Algorithmic Stability**\n",
    "\n",
    "$$P(|R_A(S) - E[R_A(S)]| \\geq \\epsilon) \\leq 2\\exp\\left(\\frac{-2\\epsilon^2}{n(\\beta + \\frac{2}{n})^2}\\right).$$\n",
    "\n",
    "To prove this fact let's look at the following expression:\n",
    "\n",
    "$$|R_A(S)-R_A(S^K)|=\\left|\\dfrac{1}{n}\\sum_{i=1}^nl_{01}(g_S(x_i),y_i)-\\dfrac{1}{n}\\sum_{i=1}^nl_{01}(g_{S^K}(x_i'),y_i')\\right|,$$\n",
    "\n",
    "where $x_i', y_i'$ are samples from the dataset $S^K$. Since we know, that the only sample which is different\n",
    "from samples $x_i, y_i$ in $S$, we can write down the following:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "|R_A(S)-R_A(S^K)|=\\left|\\dfrac{1}{n}\\sum_{i=1, i\\ne k}^n\\left(l_{01}(g_S(x_i),y_i)-l_{01}(g_{S^K}(x_i),y_i)\\right)+\\\\\n",
    "+\\dfrac{1}{n}l_{01}(g_S(x_k),y_k) -\\dfrac{1}{n}l_{01}(g_{S^K}(x_k'),y_k')\\right|\\le\\\\\n",
    "\\le\\left|\\dfrac{1}{n}\\sum_{i=1, i\\ne k}^n\\left(l_{01}(g_S(x_i),y_i)-l_{01}(g_{S^K}(x_i),y_i)\\right)\\right|+\\\\\n",
    "+\\left|\\dfrac{1}{n}l_{01}(g_S(x_k),y_k)\\right| +\\left|\\dfrac{1}{n}l_{01}(g_{S^K}(x_k'),y_k')\\right|,\n",
    "\\end{eqnarray}\n",
    "\n",
    "Using the $\\beta-$stability of the algorithm we can obtain in the form of\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left|\\left(l_{01}(g_S(x_i),y_i)-l_{01}(g_{S^K}(x_i),y_i)\\right)\\right|\\le \\beta,\n",
    "\\end{eqnarray}\n",
    "\n",
    "we can obtain\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left|\\dfrac{1}{n}\\sum_{i=1, i\\ne k}^n\\left(l_{01}(g_S(x_i),y_i)-l_{01}(g_{S^K}(x_i),y_i)\\right)\\right|\\le\\\\\n",
    "\\le \\dfrac{1}{n} \\sum_{i=1, i\\ne k}^n\\beta=\\dfrac{1}{n}(n-1)\\beta<\\beta.\n",
    "\\end{eqnarray}\n",
    "\n",
    "more over, because $l_{01}(g_S(x_i),y_i)$ and $l_{01}(g_{S^K}(x_k'),y_k')$ are limited by the value 1,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "|R_A(S)-R_A(S^K)|=\\le\\left|\\dfrac{1}{n}\\sum_{i=1, i\\ne k}^n\\left(l_{01}(g_S(x_i),y_i)-l_{01}(g_{S^K}(x_i),y_i)\\right)\\right|+\\\\\n",
    "+\\left|\\dfrac{1}{n}l_{01}(g_S(x_k),y_k)\\right| +\\left|\\dfrac{1}{n}l_{01}(g_{S^K}(x_k'),y_k')\\right|\\le\\\\\n",
    "\\le\\beta+\\dfrac{1}{n}+\\dfrac{1}{n}=\\beta+\\dfrac{2}{n}.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Thus, in the bounded difference property we can denote $c_k=\\beta+\\dfrac{2}{n}$.\n",
    "Substituting it into the McDiarmid's Inequality we get\n",
    "\n",
    "$$P(|R_A(S) - E[R_A(S)]| \\geq \\epsilon) \\leq 2\\exp\\left(\\frac{-2\\epsilon^2}{\\sum_{i=1}^n(\\beta + \\frac{2}{n})^2}\\right)=2\\exp\\left(\\frac{-2\\epsilon^2}{n(\\beta + \\frac{2}{n})^2}\\right),$$\n",
    "\n",
    "which is exactly what we want to prove.\n",
    "\n",
    "If we want to get $2\\exp\\left(\\frac{-2\\epsilon^2}{n(\\beta + \\frac{1}{n})^2}\\right)$ instead of\n",
    "$2\\exp\\left(\\frac{-2\\epsilon^2}{n(\\beta + \\frac{2}{n})^2}\\right)$, we may notice that\n",
    "\n",
    "$$\\left|\\dfrac{1}{n}l_{01}(g_S(x_k),y_k) -\\dfrac{1}{n}l_{01}(g_{S^K}(x_k'),y_k')\\right|\\le\\dfrac{1}{n},$$\n",
    "\n",
    "because $l_{01}$ is limited by the interval $[0,1]$. Everything else stays the same in the proof.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}