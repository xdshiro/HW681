{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The material presented in this notebook is for using in Introduction to Deep Learning ECE.685D course, Duke University, Fall 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data and Models in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## A summary for a general learning procedure.\n",
    "* ## Loading data:\n",
    "    * ### Common datasets, access the data by the Pytorch build-in package torchvision, torchaudio and torchtext, and then load it by torch.utils.data.DataLoader;\n",
    "    * ### Private datasets, access and load the data by adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, including \"__getitem__\" and \"__len__\".\n",
    "* ## Saving and loading models:\n",
    "    * ### save and load the entire model including the model architecture.\n",
    "    * ### save and load trained parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Training and Testing for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training,\n",
    "Train a model to fit given data, $\\{(x_i, y_i)\\}_{i=1}^n\\to \\text{Algorithm} \\to f$.\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "\n",
    "        for batch in range(trainloader):   \n",
    "            ...\n",
    "        \n",
    "### Testing,\n",
    "Evaluate how well the model match some new income data, $\\mathcal{L}(f(x), y)$.\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "\n",
    "        for batch in range(trainloader):   \n",
    "            ...\n",
    "        for batch in range(testloader):\n",
    "            ...\n",
    "\n",
    "After learning, we hope\n",
    "1. The algorithm is able to train $f$ well, saying small training errors.\n",
    "2. The trained model $f$ is able to match the pattern of new data well, saying small testing errors.\n",
    "\n",
    "With small training error, hopefully the testing error will be small.\n",
    "\n",
    "### Dataloader\n",
    "Original data -> random shuffle -> queue of minibatches for iterative training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Loading Data\n",
    "### 1. Custom the Dataloader\n",
    "(1) Define the class of data inheriting from the standard torch.utils.data.Dataset class.\n",
    "\n",
    "    a. download the online dataset or read the local dataset.\n",
    "    \n",
    "    b. define __getitem__, return index of items in the dataset.\n",
    "    \n",
    "    c. define __len__, return the size of the dataset.\n",
    "(2) Pass it to torch.utils.data.DataLoader and get the dataloader of your private dataset.\n",
    "\n",
    "### 2. Pytorch build-in packages, such as torchvision, torchtext, torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example of datasets in torchvision, MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets are subclasses of torch.utils.data.Dataset, with name of the format torchvision.datasets.\"__Name of Dataset__\"\n",
    "* They have __getitem__ and __len__ methods implemented. \n",
    "* They can all be passed to a torch.utils.data.DataLoader\n",
    "https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "\"__Name of Dataset__\" includes many datasets such as MNIST, Fashion-MNIST, ImageNet, CIFAR10, SVHN, COCO, LSUN, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform = transforms.ToTensor())\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform = transforms.ToTensor())\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 6, 0, 5])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "im = images[0:4]\n",
    "fig , ax = plt.subplots(1,4)\n",
    "for i in range(4):\n",
    "    ax[i].imshow(im[i][0], cmap='gray')\n",
    "print(labels[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Transforms are common image transformations for data augmentation. \n",
    "\n",
    "* __transforms.CenterCrop__ :  Crops the given image at the center according to the given size. \n",
    "* __transforms.ColorJitter__ :  Randomly change the brightness, contrast and saturation of an image.\n",
    "* __transforms.Grayscale__ : Convert image to grayscale.\n",
    "* __transforms.Resize__ : Resize the input image to the given size.\n",
    "* __transforms.Normalize(mean, std, inplace=False)__ : Normalize the image with mean and standard deviation. \n",
    "\n",
    "\n",
    "They can be chained together by transforms.Compose():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    CenterCrop(size=(10, 10))\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Compose([\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Saving and Loading a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to save and load?\n",
    "1. The model parameters, including weights and biases.\n",
    "2. The model architecture.\n",
    "\n",
    "Three functions to be familiar with:\n",
    "* torch.save(arg, PATH)\n",
    "* torch.load(PATH)\n",
    "* model.load_state_dict(arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is state_dict?\n",
    "model.state_dict() is simply a Python dictionary object that maps each layer to its parameter tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear.weight \t torch.Size([1, 10])\n",
      "linear.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class Linear_Reg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Linear_Reg, self).__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = Linear_Reg()\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1- Save/Load state_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear_Reg(\n",
       "  (linear): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './ckpt_state_dics.pth'\n",
    "torch.save(model.state_dict(), PATH)   # Saving\n",
    "model = Linear_Reg()\n",
    "model.load_state_dict(torch.load(PATH))  # Loading\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving a model for inference, it is only necessary to save the trained model’s learned parameters. Saving the model’s __state_dict__ with the __torch.save()__ function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2- Save/Load Entire Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wusuya/anaconda3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear_Reg. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/wusuya/anaconda3/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear_Reg(\n",
       "  (linear): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './ckpt_entire.pth'\n",
    "torch.save(model, PATH)\n",
    "\n",
    "#Model class must be defined somewhere\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
