{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Problem 5: Classification with KNN and Decision Trees**\n",
    "**5.1-5.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dTrain = pd.read_csv('carseats_train.csv', names=None)\n",
    "dTest = pd.read_csv('carseats_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_processing(data):\n",
    "    dict_ShelveLoc = {'Bad': 0, 'Medium': 0.5, 'Good' : 1}\n",
    "    dict_UrbanAndUS = {'No': 0, 'Yes' : 1}\n",
    "    return    data.replace({**dict_ShelveLoc, **dict_UrbanAndUS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copying F1 function from Q3 and testing on tree_sklearn with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7407407407407407, Recall = 0.6779661016949152, F1 = 0.70796\n",
      "F1= 0.7079646017699114\n",
      "Precision = 0.8125, Recall = 0.4406779661016949, F1 = 0.57143\n",
      "F1= 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "def F1(labels_predicted, labels_true, print_values=True):\n",
    "    TP, FP, TN, FN = ([0]*4)\n",
    "    for pr, y in zip(labels_predicted, np.array(labels_true)):\n",
    "        y = type(pr)(y)  # making sure that the types are the same\n",
    "        if pr == 1:\n",
    "            if pr == y:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            if pr == y:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    if precision + recall != 0:\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        F1 = 0\n",
    "    if print_values:\n",
    "        print(f'Precision = {precision}, Recall = {recall}, F1 ={F1: 1.5f}')\n",
    "    return F1\n",
    "\n",
    "X = data_processing(dTrain.iloc[:, 1:])\n",
    "y = dTrain['Sales']\n",
    "X_test = data_processing(dTest.iloc[:, 1:])\n",
    "y_test = dTest['Sales']\n",
    "tree_sklearn = DecisionTreeClassifier(criterion='entropy', splitter='best',\n",
    "                                     min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None,\n",
    "                                     max_depth=None, max_features=None)\n",
    "tree_sklearn.fit(X, y)\n",
    "print('F1=', F1(tree_sklearn.predict(X_test), y_test))\n",
    "tree_sklearn = DecisionTreeClassifier(criterion='entropy', splitter='best',\n",
    "                                     min_samples_split=4, min_samples_leaf=1, max_leaf_nodes=8,\n",
    "                                     max_depth=None, max_features=3)\n",
    "tree_sklearn.fit(X, y)\n",
    "print('F1=', F1(tree_sklearn.predict(X_test), y_test))\n",
    "### Plotting the tree if we want ###\n",
    "# text_representation = tree.export_text(tree_sklearn, feature_names=list(X.columns.values))\n",
    "# print(text_representation)\n",
    "# fig = plt.figure(figsize=(50,50))\n",
    "# _ = tree.plot_tree(tree_sklearn, feature_names=X.columns.values, class_names=['Sold_1', 'Not_0'], filled=True)\n",
    "# fig.savefig('.\\\\tree_sklearn.png')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Testing chefboost package. F1 value. We see, that cheefbost strongly overfits data, so F1 values on test data\n",
    "are kinda low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "C4.5  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  5.521518707275391  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  79.43262411347517 % on  282  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[55, 8], [50, 169]]\n",
      "Precision:  87.3016 %, Recall:  52.381 %, F1:  65.4762 %\n",
      "Precision = 0.8846153846153846, Recall = 0.3898305084745763, F1 = 0.54118\n",
      "F1= 0.5411764705882353\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CART  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  3.938645362854004  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  97.16312056737588 % on  282  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[100, 3], [5, 174]]\n",
      "Precision:  97.0874 %, Recall:  95.2381 %, F1:  96.1539 %\n",
      "Precision = 0.6607142857142857, Recall = 0.6271186440677966, F1 = 0.64348\n",
      "F1= 0.6434782608695652\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CHAID  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  5.422708511352539  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  92.19858156028369 % on  282  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[92, 9], [13, 168]]\n",
      "Precision:  91.0891 %, Recall:  87.619 %, F1:  89.3204 %\n",
      "Precision = 0.6551724137931034, Recall = 0.6440677966101694, F1 = 0.64957\n",
      "F1= 0.6495726495726496\n"
     ]
    }
   ],
   "source": [
    "from chefboost import Chefboost as chef\n",
    "# otherwise chef can only use regression\n",
    "dTrain_new =dTrain.astype({'Sales' : str})\n",
    "dTest_new = dTest.astype({'Sales' : str})\n",
    "# config = {'algorithm':'CART', 'enableGBM': True, 'epochs': 7, 'learning_rate': 1}\n",
    "config = {'algorithm':'C4.5'}\n",
    "for config in [{'algorithm':'C4.5'}, {'algorithm':'CART'}, {'algorithm':'CHAID'}]:\n",
    "    # config = {'enableGBM': True, 'epochs': 7, 'learning_rate': 1, 'max_depth': 5}\n",
    "    tree_chef = chef.fit(dTrain_new, config, target_label = 'Sales')\n",
    "    # chef.evaluate(tree_chef, dTest_new, task='test', target_label = 'Sales')\n",
    "    y_predicted = np.zeros(len(dTest_new))\n",
    "    for i in range(len(dTest_new)):\n",
    "        y_predicted[i] = chef.predict(tree_chef, param=(dTest_new.iloc[i, 1:]))\n",
    "    print('F1=', F1(y_predicted, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "K-Fold cross validation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def K_Fold_cross_validation(method_predict, validation, X, y, K_values, K_name, fold_coeff=10, XTDEL=None, YTDEL=None, **config):\n",
    "    # len_K - length of the fold_coeff-1 folds. The last one can be a little bit bigger to cover all data left\n",
    "    len_K = len(X) // fold_coeff\n",
    "    fullValidation = np.zeros((fold_coeff, len(K_values)))\n",
    "    for i in range(fold_coeff - 1):\n",
    "        XTraining = X.drop(X.index[len_K*i:len_K*(i+1)], axis=0)\n",
    "        yTraining = y.drop(y.index[len_K*i:len_K*(i+1)], axis=0)\n",
    "        XValid = X.iloc[len_K*i:len_K*(i+1), :]\n",
    "        yValid = y.iloc[len_K*i:len_K*(i+1)]\n",
    "        for j, K in enumerate(K_values):\n",
    "            K_name_dict = {K_name : K}\n",
    "            method = method_predict(**config, **K_name_dict)\n",
    "            method.fit(XTraining, yTraining)\n",
    "            fullValidation[i, j] = validation(method.predict(XValid), np.array(yValid), print_values=False)\n",
    "    # last fold\n",
    "    XTraining = X.drop(X.index[len_K*(fold_coeff - 1):], axis=0)\n",
    "    yTraining = y.drop(y.index[len_K*(fold_coeff - 1):], axis=0)\n",
    "    XValid = X.iloc[len_K*(fold_coeff - 1):, :]\n",
    "    yValid = y.iloc[len_K*(fold_coeff - 1):]\n",
    "    for j, K in enumerate(K_values):\n",
    "        K_name_dict = {K_name : K}\n",
    "        method = method_predict(**config, **K_name_dict)\n",
    "        method.fit(XTraining, yTraining)\n",
    "        fullValidation[fold_coeff - 1, j] = validation(method.predict(XValid), np.array(yValid), print_values=False)\n",
    "    allKValidation = np.sum(fullValidation, axis=0) / fold_coeff\n",
    "    print('mean K_Fold validation values')\n",
    "    print(allKValidation)\n",
    "    print(f'The best K value is # {np.argmax(allKValidation)}'\n",
    "          f' with value K={K_values[np.argmax(allKValidation)]}')\n",
    "    return allKValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using K-Fold for the parameter 'max_features' for sklearn.tree.\n",
    "10 Folds is used but it seems like it's too many for such small data. F1 values are too small, slightly above 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean K_Fold validation values\n",
      "[0.5260308  0.55724253 0.54691079 0.56041792 0.56930873 0.59578548\n",
      " 0.57399338 0.50595559 0.56970607 0.53776525]\n",
      "The best K value is # 5 with value K=6\n",
      "Precision = 0.8, Recall = 0.7457627118644068, F1 = 0.77193\n",
      "F1= 0.7719298245614035\n"
     ]
    }
   ],
   "source": [
    "K_name = 'max_features'\n",
    "K_values = [1,2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "fold_coeff = 10\n",
    "method_predict = DecisionTreeClassifier\n",
    "validation = F1\n",
    "config = {'random_state': 10, 'criterion' : 'entropy', 'splitter' : 'best',\n",
    "          'min_samples_split'  : 2, 'min_samples_leaf'  : 1,\n",
    "          'max_leaf_nodes' : None}\n",
    "kFold = K_Fold_cross_validation(method_predict, validation, X, y,\n",
    "                                K_values, K_name, fold_coeff, X_test, y_test, **config)\n",
    "\n",
    "# Training with the best 'max_features' parameter.\n",
    "tree_sklearn = DecisionTreeClassifier(**config, max_features=K_values[np.argmax(kFold)])\n",
    "tree_sklearn.fit(X, y)\n",
    "print('F1=', F1(tree_sklearn.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using K-Fold for the parameter 'algorithm' for Chefboost. Here I have to write a wrapper for Chefboost because it\n",
    "works with the different data representation, compared to sklearn. 5 folds in work are showed  (K=5 since it's slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "ID3  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  4.080012083053589  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  94.69026548672566 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[84, 11], [1, 130]]\n",
      "Precision:  88.4211 %, Recall:  98.8235 %, F1:  93.3333 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "C4.5  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  3.637967586517334  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  78.76106194690266 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[44, 7], [41, 134]]\n",
      "Precision:  86.2745 %, Recall:  51.7647 %, F1:  64.7059 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CART  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  4.379726409912109  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  98.23008849557522 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[82, 1], [3, 140]]\n",
      "Precision:  98.7952 %, Recall:  96.4706 %, F1:  97.6191 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CHAID  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  9.191888809204102  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  89.38053097345133 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[67, 6], [18, 135]]\n",
      "Precision:  91.7808 %, Recall:  78.8235 %, F1:  84.8101 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "ID3  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  7.084787368774414  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  96.90265486725664 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[79, 2], [5, 140]]\n",
      "Precision:  97.5309 %, Recall:  94.0476 %, F1:  95.7576 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "C4.5  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  4.970527410507202  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  82.30088495575221 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[53, 9], [31, 133]]\n",
      "Precision:  85.4839 %, Recall:  63.0952 %, F1:  72.6027 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CART  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  7.964414834976196  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  96.46017699115045 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[78, 2], [6, 140]]\n",
      "Precision:  97.5 %, Recall:  92.8571 %, F1:  95.1219 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CHAID  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  8.07904863357544  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  88.05309734513274 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[60, 3], [24, 139]]\n",
      "Precision:  95.2381 %, Recall:  71.4286 %, F1:  81.6327 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "ID3  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  7.094144582748413  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  96.46017699115045 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[79, 2], [6, 139]]\n",
      "Precision:  97.5309 %, Recall:  92.9412 %, F1:  95.1808 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "C4.5  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  2.488813638687134  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  80.53097345132744 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[41, 0], [44, 141]]\n",
      "Precision:  100.0 %, Recall:  48.2353 %, F1:  65.0794 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CART  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  5.287153244018555  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  96.01769911504425 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[76, 0], [9, 141]]\n",
      "Precision:  100.0 %, Recall:  89.4118 %, F1:  94.41 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CHAID  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  7.1630942821502686  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  86.72566371681415 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[62, 7], [23, 134]]\n",
      "Precision:  89.8551 %, Recall:  72.9412 %, F1:  80.5195 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "ID3  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  7.18835973739624  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  95.13274336283186 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[84, 8], [3, 131]]\n",
      "Precision:  91.3043 %, Recall:  96.5517 %, F1:  93.8547 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "C4.5  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  6.217751979827881  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  80.08849557522124 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[60, 18], [27, 121]]\n",
      "Precision:  76.9231 %, Recall:  68.9655 %, F1:  72.7273 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CART  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  7.870629787445068  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  96.01769911504425 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[85, 7], [2, 132]]\n",
      "Precision:  92.3913 %, Recall:  97.7011 %, F1:  94.972 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CHAID  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  10.143669366836548  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  87.61061946902655 % on  226  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[73, 14], [14, 125]]\n",
      "Precision:  83.908 %, Recall:  83.908 %, F1:  83.908 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "ID3  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  6.436746120452881  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  95.98214285714286 % on  224  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[77, 7], [2, 138]]\n",
      "Precision:  91.6667 %, Recall:  97.4684 %, F1:  94.4786 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "C4.5  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  3.524055004119873  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  78.57142857142857 % on  224  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[41, 10], [38, 135]]\n",
      "Precision:  80.3922 %, Recall:  51.8987 %, F1:  63.0769 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CART  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  6.698387384414673  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  97.32142857142857 % on  224  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[75, 2], [4, 143]]\n",
      "Precision:  97.4026 %, Recall:  94.9367 %, F1:  96.1538 %\n",
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CHAID  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  4.71421217918396  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  88.83928571428571 % on  224  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[58, 4], [21, 141]]\n",
      "Precision:  93.5484 %, Recall:  73.4177 %, F1:  82.2695 %\n",
      "mean K_Fold validation values\n",
      "[0.50771682 0.54033641 0.59155328 0.59177979]\n",
      "The best K value is # 3 with value K={'algorithm': 'CHAID', 'enableRandomForest': False, 'num_of_trees': 5, 'enableMultitasking': False, 'enableGBM': False, 'epochs': 10, 'learning_rate': 1, 'max_depth': 3, 'enableAdaboost': False, 'num_of_weak_classifier': 4, 'enableParallelism': True, 'num_cores': 8}\n"
     ]
    }
   ],
   "source": [
    "class chefMethod_helper():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.fit = self.chefFit_helper\n",
    "        self.predict = self.chefPredict_helper\n",
    "\n",
    "    def chefFit_helper(self, X , y):\n",
    "        Xy = pd.concat([y, X], axis=1)\n",
    "        self.method = chef.fit(Xy, **self.kwargs)\n",
    "\n",
    "    def chefPredict_helper(self, X):\n",
    "        y_predicted = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            y_predicted[i] = chef.predict(self.method, param=(X.iloc[i, :]))\n",
    "        return y_predicted\n",
    "\n",
    "dTrain_new = dTrain.astype({'Sales' : str})\n",
    "X = data_processing(dTrain_new.iloc[:, 1:])\n",
    "y = dTrain_new['Sales']\n",
    "\n",
    "K_name = 'config'\n",
    "K_values = [{'algorithm':'ID3'}, {'algorithm':'C4.5'}, {'algorithm':'CART'}, {'algorithm':'CHAID'}]\n",
    "fold_coeff = 5\n",
    "method_predict = chefMethod_helper\n",
    "validation = F1\n",
    "config = {'target_label' : 'Sales'}\n",
    "kFold = K_Fold_cross_validation(method_predict, validation, X, y,\n",
    "                                K_values, K_name, fold_coeff, X_test, y_test, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training with the best 'algorithm' parameter, which is CHAID according to K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]:  8 CPU cores will be allocated in parallel running\n",
      "CHAID  tree is going to be built...\n",
      "-------------------------\n",
      "finished in  6.160266876220703  seconds\n",
      "-------------------------\n",
      "Evaluate  train set\n",
      "-------------------------\n",
      "Accuracy:  92.19858156028369 % on  282  instances\n",
      "Labels:  ['1' '0']\n",
      "Confusion matrix:  [[92, 9], [13, 168]]\n",
      "Precision:  91.0891 %, Recall:  87.619 %, F1:  89.3204 %\n",
      "Precision = 0.6551724137931034, Recall = 0.6440677966101694, F1 = 0.64957\n",
      "F1= 0.6495726495726496\n"
     ]
    }
   ],
   "source": [
    "tree_chef = chef.fit(dTrain_new, config = K_values[np.argmax(kFold)], target_label = 'Sales')\n",
    "y_predicted = np.zeros(len(dTest_new))\n",
    "for i in range(len(dTest_new)):\n",
    "    y_predicted[i] = chef.predict(tree_chef, param=(dTest_new.iloc[i, 1:]))\n",
    "print('F1=',F1(y_predicted, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**5.3** Implementation the KNN algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def distance_euclidean(x, y):\n",
    "    xNp, yNp = np.array(x), np.array(y)\n",
    "    return np.sqrt(np.sum((xNp - yNp) ** 2))\n",
    "\n",
    "def distance_cosine(x, y):\n",
    "    xNp, yNp = np.array(x), np.array(y)\n",
    "    xDotY = np.sum(xNp * yNp)\n",
    "    xMod = np.sqrt(np.sum(xNp ** 2))\n",
    "    yMod = np.sqrt(np.sum(yNp ** 2))\n",
    "    cosSimilarity = xDotY / xMod / yMod\n",
    "    return 1 - cosSimilarity\n",
    "\n",
    "def distance_from_array(x, Y, distance=distance_euclidean):\n",
    "    return np.array([distance(x, y) for y in Y])\n",
    "\n",
    "def normalization_array(array, return_coeff=True):\n",
    "    arrayMod = np.sqrt(np.sum(array ** 2, axis=0))\n",
    "    if return_coeff:\n",
    "        return np.divide(array, arrayMod), arrayMod\n",
    "    else:\n",
    "        return np.divide(array, arrayMod)\n",
    "\n",
    "def KNN(point, data, K,distance=distance_euclidean, label=None, binary=True):\n",
    "    if label is None:\n",
    "        label = data.columns[0]\n",
    "    elif isinstance(label, int):\n",
    "        label = data.columns[label]\n",
    "    X = data.drop(label, axis=1).values\n",
    "    y = data[label].values\n",
    "    # normalization\n",
    "    X, pointMod = normalization_array(X)\n",
    "    point = point / pointMod\n",
    "    dist = distance_from_array(point, X, distance=distance)\n",
    "    indexDist = dist.argsort()[:K]\n",
    "    # print(X[indexDist], y[indexDist])\n",
    "    if binary:\n",
    "        if y[indexDist].mean() >= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return y[indexDist].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Testing our own KNN with 2 different distances: euclidean and cosine. Also different number of neighbors K.\n",
    "The best performance for euclidean metrics achieved by setting K=4, for cosine with K=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: <function distance_euclidean at 0x0000018D17B18F70>\n",
      "K=1, F1=0.613861386138614\n",
      "K=2, F1=0.7086614173228345\n",
      "K=3, F1=0.673076923076923\n",
      "K=4, F1=0.7377049180327869\n",
      "K=5, F1=0.6464646464646464\n",
      "K=6, F1=0.6909090909090908\n",
      "K=7, F1=0.6999999999999998\n",
      "K=8, F1=0.7037037037037036\n",
      "K=9, F1=0.6804123711340205\n",
      "K=10, F1=0.6915887850467289\n",
      "K=11, F1=0.6999999999999998\n",
      "Distance: <function distance_cosine at 0x0000018D18592C10>\n",
      "K=1, F1=0.607843137254902\n",
      "K=2, F1=0.732824427480916\n",
      "K=3, F1=0.7047619047619047\n",
      "K=4, F1=0.7130434782608696\n",
      "K=5, F1=0.7047619047619047\n",
      "K=6, F1=0.6956521739130435\n",
      "K=7, F1=0.6990291262135923\n",
      "K=8, F1=0.6788990825688074\n",
      "K=9, F1=0.6990291262135923\n",
      "K=10, F1=0.7027027027027027\n",
      "K=11, F1=0.6923076923076924\n"
     ]
    }
   ],
   "source": [
    "def data_processing(data):\n",
    "    dict_ShelveLoc = {'Bad': 0, 'Medium': 0.5, 'Good' : 1}\n",
    "    dict_UrbanAndUS = {'No': 0, 'Yes' : 1}\n",
    "    return    data.replace({**dict_ShelveLoc, **dict_UrbanAndUS})\n",
    "\n",
    "dTrain = data_processing(pd.read_csv('carseats_train.csv', names=None))\n",
    "dTest = data_processing(pd.read_csv('carseats_test.csv'))\n",
    "points = dTest.iloc[:, 1:].values\n",
    "# KNN(point, dTrain, 3, label=0)\n",
    "y_predicted = np.zeros(len(dTest))\n",
    "for distance in [distance_euclidean, distance_cosine]:\n",
    "    print(f'Distance: {distance}')\n",
    "    for K in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]:\n",
    "        for i, point in enumerate(points):\n",
    "            y_predicted[i] = KNN(point, dTrain, distance=distance, K=K, label=0)\n",
    "        print(f'K={K}, F1={F1(y_predicted, np.array(y_test), print_values=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4**\n",
    "(all the evaluations are based on F1 values)\n",
    "The worst performance is shown by Chefboost implementation of decision trees. I could not get a high\n",
    "performance even trying all the available algorithms and I haven't found the way to adjust other parameters.\n",
    "Seems like it overfit a lot.  F1<0.7.\n",
    "It's also the slowest one.\n",
    "\n",
    "The best performance was reached by sklearn.tree.DecisionTreeClassifier with \"max_features\"=5. F1=0.77\n",
    "\n",
    "KNN algorithm is close to sklearn.tree but it's still performs worse: F1=0.74. (But I believe, with some\n",
    "weights tuning it can do better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**5.5**\n",
    "In K-fold cross-validation we divide our data into K \"almost\" equally sized subsets (where people often use K=10)\n",
    "and repeat training/testing model K times, each time rotation the test fold. However, in  Leave-One-Out\n",
    "cross-calidation our test fold consists of only 1 data sample and the rest of the data is used for training.\n",
    "This process is repeated N times (where N is the number of data points) and the \"test fold\" is rotating\n",
    "through all the data points. Basically, it's K-fold with K=N.\n",
    "\n",
    "Disadvantages of the LOOCV:\n",
    "\n",
    "1) If the data is big, it's very time and resource consuming process since it requires N trainings for your algorithm.\n",
    "\n",
    "2) LOOCV can overfit a lot since it's trained on almost all the data and is being validated on only 1 sample\n",
    "(as a result it can also have a very high variability in predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6**\n",
    "I believe, that F1 is a good metric for that problem (especially compared to Accuracy) because the data is\n",
    "quite imbalance (there are almost 2 times less sales).\n",
    "\n",
    "F1 fully confirmed the \"feeling\" (described in **5.4**: scrlen the best, KNN 2nd, Chefboost the worst)\n",
    "I had about all the algorithms I was studying in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "standBy = True\n",
    "While True:\n",
    "    if standBy:\n",
    "        if спец жест:\n",
    "            standBy = False\n",
    "    else:\n",
    "        do everything\n",
    "        if спец жест:\n",
    "            standBy = True\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}